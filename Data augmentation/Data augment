def feature_extraction(timestep, previous) : '''
PURPOSE: Calculate the features for a machine learning model within the context METHOD: Use the predictors and the calculation methodology defined in Knaff 2013 INPUT:	timestep - current dictionary of features in the hurricane object format
previous  -  previous  timestep  dictionary  of  features  in  the  hurricane  obj OUTPUT: Dictionary of features

timestep = {
'lat' : float,
'long' : float,
'max-wind' : float,
'entry-time' : datetime
}
'''
features = {
'lat' : timestep['lat'],
'long' : timestep['long'],
'max_wind' : timestep['max_wind'],
'delta_wind' : (timestep['max_wind'] - previous['max_wind']) / # Calculated
((timestep['entry_time'] - previous['entry_time']).total_seconds() / 432 'min_pressure' : timestep['min_pressure'],
'zonal_speed' : (timestep['lat'] - previous['lat'])/ # Calculated from track
((timestep['entry_time'] - previous['entry_time']).total_seconds() / 360 'meridonal_speed'  :  (timestep['long']  -  previous['long'])/#  Calculated  from
((timestep['entry_time'] - previous['entry_time']).total_seconds() / 360 'year' : timestep['entry_time'].year,
'month'  :  timestep['entry_time'].month,
'day' : timestep['entry_time'].day, 'hour' : timestep['entry_time'].hour,
}
return features

def  storm_x_y(storm,  timesteps  =  1,  lag  =  24)  : '''
PURPOSE: Create independent and dependent samples for a machine learning model b
METHOD: Use the HURDAT2 database and a hurricane object as defined in hurricane-
INPUT:	storm - hurricane object

timesteps - (default = 1) number of timesteps to calculate
include_none - (default = False) Boolean for including  None  in test data lag - (default = 24) lag in hours for the dependent variables up to 5 da
OUTPUT: Dictionary with independent (x) and dependent (y) values. '''
x = []




# Create testing data structure with a dictionary
times  =  [time  *  lag  for  time  in  range(1,  (120  //  lag)  +  1)] 
#  Begin  at  lag  hours
y = dict([(time,[]) for time in times])



# Sort by entry time
entries  =  [entry[1]  for  entry  in  sorted(storm.entries.items())]

for index in range(len(entries)) :
if index < timesteps : # Flag for insufficient initial time steps
continue

# If we're not including None values, check to see if there will be any
if None in [storm.entries.get(entries[index]['entry_time'] +
datetime.timedelta(hours = future)) for fut



# Calculate time steps and their features for independent values






# Calculate time steps and their features for dependent values









def shape(hurricanes, timesteps, remove_missing = True) : '''
PURPOSE: Shape our data for input into machine learning models
METHOD: Use a numpy array to shape into (samples, timesteps, features)
INPUT:	hurricanes - dictionary of hurricane objects timesteps - number of timesteps for the shape
remove_missing - boolean indicating whether the algorithm will disregard OUTPUT: numpy array of shape (samples, timesteps, 11) where 11 is the number of '''
x  = []
y  = []
lag = 24 # lag time in hours
precision  =  np.float64  #  defines  the  precision  of  our  data  type
times  =  [time  *  lag  for  time  in  range(1,  (120  //  lag)  +  1)]  #  Begin  at  lag  hours
count = 0
for hurricane in hurricanes.values() : count += 1
result = storm_x_y(hurricane, timesteps, lag)
if result is None :
continue
# Extract only the values from the strom features using our specified precis
hurricane_x  =  np.array(
[[list(sample[1][0].values())  for  sample  in  x]  for  x  in  result['x']], dtype = precision)
hurricane_y  =  np.array(
[[list(result['y'][time][index].values()) for time in times] for index i
dtype = precision)
# Disregard if algorithm requires no missing values
if remove_missing :
if  (len(np.where(np.isnan(hurricane_x))[0])  >  0)  or  (len(np.where(np.isn
continue
# Add to our results x.extend(hurricane_x) y.extend(hurricane_y)
print("Feature engineered {}/{} hurricanes for {} timestep(s)".format(count, print("\nDone feature engineering hurricanes.")

return  {'x':  np.array(x),  'y':  np.array(y)}
def scaler(processed_data, hurricanes) : '''
PURPOSE: Scale our data using the RobustScaler method from the sklearn library
METHOD:  Generate  data  using  1  timesteps  and  then  remove  the  NaN  or  None  types  to INPUT:	hurricanes - dictionary of hurricane objects
processed_data - dictionary of x and y values of data produced by shape( OUTPUT: 1) Scaled processed_data using RobustScaler
2) RobustScaler object fit with appropriate data
'''
print("Scaling Data . . . (1 timestep for unqiue data)")
# Create our scaler
unqiue_data = shape(hurricanes, timesteps = 1)
x  =  np.reshape(unqiue_data['x'],  (unqiue_data['x'].shape[0],  -1)) x  =  np.delete(x,  np.where(np.isnan(x))[0],  0)
scaler = RobustScaler()
scaler.fit(x)

# Scale our data
for index in range(len(processed_data['x'])) :
 

 
